# 代码改动日志 / Change Log

## Version 6.0 - 防遗忘优化版 (当前版本)

**改动日期:** 2026-02-05

**v5.0 问题诊断 (2000轮训练):**

训练 2000 轮后出现 **灾难性遗忘 (Catastrophic Forgetting)**：
- Peak avg: 39.3 (episode 690)
- Best score: 710 (episode 1280)
- Final avg: 11.1 (episode 2000) - 性能崩溃！

**根本原因:**
1. Replay Buffer 太小 (10000) → 好经验被覆盖
2. Epsilon 衰减太快 (0.995) → ep950 后几乎无探索
3. 无早停机制 → 性能下降后仍继续训练
4. 只保存 best_score 模型 → 忽略了 best_avg 模型

---

### 改动 1: 增大 Replay Buffer

**文件:** `agent/agent.py`, `train.py`

| 参数 | v5.0 | v6.0 | 改动原因 |
|------|------|------|----------|
| `buffer_size` | 10000 | **50000** | 保留更多好经验 |

**之前版本的缺点:**
- 10000 条经验约等于 ~40 个高分回合
- 好经验很快被失败经验覆盖
- 网络在坏经验上训练导致遗忘

**改动效果:**
- 50000 条经验可保留 ~200 个回合
- 好经验保留时间更长
- 减少遗忘风险

---

### 改动 2: 减慢 Epsilon 衰减

**文件:** `train.py`

| 参数 | v5.0 | v6.0 | 改动原因 |
|------|------|------|----------|
| `epsilon_decay` | 0.995 | **0.998** | 保持更长探索期 |

**Epsilon 到达 0.01 的时间:**
- v5.0: ~920 episodes
- v6.0: ~2300 episodes

**改动效果:**
- 更长时间保持探索
- 避免过早陷入局部最优
- 能发现更多好策略

---

### 改动 3: 添加早停机制

**文件:** `train.py`

```python
# 早停条件
if episode >= 500 and episodes_since_peak >= early_stop_patience:
    if avg_score < peak_avg_score * early_stop_threshold:
        # 触发早停
        early_stopped = True
        break
```

**参数:**
- `early_stop_patience`: 200 episodes (等待期)
- `early_stop_threshold`: 0.5 (50% of peak)

**工作原理:**
1. 记录峰值平均分 (peak_avg_score)
2. 如果 200 轮内 avg 没有超过峰值
3. 且当前 avg < 50% * 峰值
4. 触发早停，保护已学策略

---

### 改动 4: 保存最佳平均分模型

**文件:** `train.py`

**之前版本:**
```python
# 只保存最高分模型
if score > best_score:
    agent.save("best_model.pth")
```

**现在版本:**
```python
# 同时保存最佳平均分模型
if avg_score > best_avg_score:
    agent.save("best_avg_model.pth")  # 新增！
```

**改动原因:**
- 最高分可能是偶然的 (运气好)
- 最佳平均分代表稳定的好策略
- `best_avg_model.pth` 更适合实际使用

---

### 改动 5: 支持 Prioritized Experience Replay (可选)

**文件:** `agent/agent.py`

```python
# 使用 PER
agent = DQNAgent(use_per=True)

# 训练时更新优先级
td_errors = |Q_predicted - Q_target|
memory.update_priorities(indices, td_errors)
```

**PER 工作原理:**
1. 每个经验有一个优先级 (TD error)
2. TD error 大的经验被采样更多
3. 好经验（学习价值高）被重复学习

**使用方式:**
```bash
python train.py --episodes 1000 --per
```

---

### v6.0 超参数汇总

| 参数 | v5.0 | v6.0 | 改动原因 |
|------|------|------|----------|
| `buffer_size` | 10000 | **50000** | 防止好经验丢失 |
| `epsilon_decay` | 0.995 | **0.998** | 更长探索期 |
| `early_stop_patience` | 无 | **200** | 防止过度训练 |
| `early_stop_threshold` | 无 | **0.5** | 性能下降阈值 |
| `use_per` | 无 | **可选** | 优先经验回放 |

---

### v6.0 命令行选项

```bash
# 标准训练 (带早停)
python train.py --episodes 1000

# 使用 PER
python train.py --episodes 1000 --per

# 禁用早停 (完整训练)
python train.py --episodes 2000 --no-early-stop

# 自定义早停耐心值
python train.py --episodes 1000 --patience 300
```

---

### v6.0 输出模型

| 模型文件 | 用途 |
|----------|------|
| `best_model.pth` | 最高分模型 (可能不稳定) |
| `best_avg_model.pth` | **推荐使用** - 最稳定的高性能模型 |
| `final_model.pth` | 最终模型 (可能已遗忘) |

---

### 待验证

运行 v6.0 训练后需要验证:
1. 早停是否在性能下降前触发
2. 最终 avg 是否保持在峰值附近
3. best_avg_model 是否比 best_model 更稳定

---

## Version 5.0 - 开源项目优化版 (已废弃)

**改动日期:** 2026-02-05

**v4.0 训练结果分析 (1000轮):**

| 指标 | 500轮 | 1000轮 | 问题 |
|------|--------|---------|------|
| Best Score | 210 | 210 | 未提升 |
| Avg Score | 9.4 | 6.4 | 下降！ |
| 0分回合 | ~70% | ~80% | 更多失败 |

**问题诊断:**
1. 训练不稳定 - 500轮后 best score 不再提升
2. 平均分下降 - 1000轮的平均分反而比500轮低
3. 策略不稳定 - 大量0分回合，学会的策略容易丢失

---

### 开源项目研究

参考了以下成功的开源项目:

#### 1. aome510/chrome-dino-game-rl
**链接:** https://github.com/aome510/chrome-dino-game-rl

**关键发现:**
> "The reward is defined to be the number of obstacles that the agent passes"

- 奖励设计极其简单：通过障碍=1，死亡=-1，其他=0.01
- 使用 Double DQN 减少 Q 值过估计
- 训练 2000+ 轮才稳定

#### 2. hfahrudin/trex-DQN
**链接:** https://github.com/hfahrudin/trex-DQN

**关键发现:**
> "shorter air time = more learning opportunities"

- 更快的跳跃物理（高跳跃力+高重力）
- 让智能体每秒能尝试更多跳跃
- 加速经验积累

#### 3. 其他通用 DQN 最佳实践
- **Huber Loss** (SmoothL1Loss) 比 MSE 更稳定
- **小网络** (128-64) 避免过拟合
- **target_update_freq=100** 更频繁更新

---

### 改动 1: 实现 Double DQN

**文件:** `agent/agent.py`

**之前版本 (v4.0) - Standard DQN:**
```python
# 目标网络同时选择和评估动作
next_q = self.target_net(next_states).max(1)[0]
```

**现在版本 (v5.0) - Double DQN:**
```python
# 策略网络选择动作
next_actions = self.policy_net(next_states).argmax(1, keepdim=True)
# 目标网络评估该动作
next_q = self.target_net(next_states).gather(1, next_actions).squeeze()
```

**之前版本的缺点:**
1. Standard DQN 使用同一网络选择和评估动作
2. 导致 Q 值过估计 (overestimation)
3. Q 值过估计会让策略不稳定

**改动原因:**
1. **Double DQN 论文** (van Hasselt et al., 2015) 证明分离选择和评估可以减少过估计
2. 开源项目普遍使用 Double DQN
3. 更稳定的 Q 值 = 更稳定的策略

---

### 改动 2: 使用 Huber Loss

**文件:** `agent/agent.py`

**之前版本 (v4.0):**
```python
self.criterion = nn.MSELoss()
```

**现在版本 (v5.0):**
```python
self.criterion = nn.SmoothL1Loss()  # Huber loss
```

**之前版本的缺点:**
1. MSE 对离群值敏感
2. 大误差会产生大梯度，可能导致训练不稳定
3. RL 中奖励波动大，容易产生离群样本

**改动原因:**
1. Huber loss 在误差小时像 MSE，误差大时像 MAE
2. 对离群值更鲁棒
3. 是 DQN 训练的标准做法

---

### 改动 3: 简化网络结构

**文件:** `agent/dqn_model.py`

**之前版本 (v4.0):**
```python
self.fc1 = nn.Linear(state_size, 256)
self.fc2 = nn.Linear(256, 128)
self.fc3 = nn.Linear(128, 64)
self.fc4 = nn.Linear(64, action_size)
```

**现在版本 (v5.0):**
```python
self.fc1 = nn.Linear(state_size, 128)
self.fc2 = nn.Linear(128, 64)
self.fc3 = nn.Linear(64, action_size)
```

**之前版本的缺点:**
1. 网络太大 (4层，256-128-64)
2. 对于简单任务容易过拟合
3. 更多参数需要更多数据才能训练好

**改动原因:**
1. **奥卡姆剃刀**: 简单任务用简单网络
2. aome510 项目使用 128-64 网络取得好效果
3. 减少过拟合风险

---

### 改动 4: 更快的跳跃物理

**文件:** `game/constants.py`

| 参数 | v4.0 | v5.0 | 改动原因 |
|------|------|------|----------|
| `JUMP_VELOCITY` | -15 | **-18** | 跳得更高更快 |
| `GRAVITY` | 0.8 | **1.2** | 落地更快 |
| `OBSTACLE_SPEED_INIT` | 6 | **5** | 初始更慢 |
| `OBSTACLE_GAP_MIN` | 400 | 400 | 保持不变 |
| `OBSTACLE_GAP_MAX` | 700 | **600** | 稍微紧凑 |

**之前版本的缺点:**
1. 跳跃时间 ~1.5 秒，每次跳跃周期太长
2. 每集只能尝试 5-10 次跳跃
3. 经验积累慢

**改动原因:**
1. **hfahrudin 项目的关键发现**: 更短的空中时间 = 更多学习机会
2. 跳跃时间 ~0.75 秒，每集可尝试更多跳跃
3. 加速经验积累，加快学习

---

### 改动 5: 简化状态空间

**文件:** `game/dino_game.py` - `get_state()` 方法

**之前版本 (v4.0) - 5维:**
```python
state[0] = dist / 400                    # 距离
state[1] = height / OBSTACLE_MAX_HEIGHT  # 高度
state[2] = 1.0 if is_close else 0.0      # 是否接近
state[3] = 1.0 if self.dino.is_jumping   # 是否跳跃
state[4] = self.speed / OBSTACLE_SPEED_MAX  # 速度
```

**现在版本 (v5.0) - 4维 + urgency signal:**
```python
state[0] = max(0, min(1, dist / 400))        # 距离 (0-1)
state[1] = max(0, 1 - dist / 150) if dist > 0 else 1.0  # ⭐ Urgency Signal
state[2] = 1.0 if self.dino.is_jumping else 0.0
state[3] = self.dino.velocity_y / 20.0       # 垂直速度
```

**关键创新 - Urgency Signal:**
- 距离 > 150: urgency = 0 (不急)
- 距离 = 75: urgency = 0.5 (需要注意)
- 距离 = 0: urgency = 1 (必须跳!)

**之前版本的缺点:**
1. `is_close` 是二值特征 (0/1)，过于粗糙
2. 没有渐变的紧急程度
3. 障碍物高度对于只有跳跃的游戏意义不大

**改动原因:**
1. **Urgency signal** 提供连续的"紧急程度"信号
2. 让神经网络更容易学习"距离越近越要跳"
3. 移除不必要的特征（高度、速度）

---

### 改动 6: 极简奖励函数

**文件:** `game/dino_game.py` - `_calculate_reward()` 方法

**之前版本 (v4.0):**
```python
def _calculate_reward(self, action, collision, passed, old_state):
    if collision:
        return -1.0
    if passed > 0:
        return 5.0  # 通过奖励

    reward = 0.1  # 存活奖励
    if is_close and action == 1 and was_on_ground:
        reward += 0.5  # 正确跳跃
    return reward
```

**现在版本 (v5.0):**
```python
def _calculate_reward(self, collision, passed):
    if collision:
        return -1.0  # 死亡
    if passed > 0:
        return 1.0   # 通过障碍
    return 0.01      # 存活
```

**之前版本的缺点:**
1. 奖励幅度不平衡 (死亡-1, 通过+5, 存活+0.1)
2. "正确跳跃"奖励需要复杂的条件判断
3. 可能给出误导性奖励

**改动原因:**
1. **aome510 项目的核心思想**: "reward = number of obstacles passed"
2. 简单的奖励 = 清晰的学习信号
3. 让智能体自己发现"何时跳跃"而不是被引导

---

### v5.0 超参数汇总

| 参数 | v4.0 | v5.0 | 改动原因 |
|------|------|------|----------|
| `state_size` | 5 | **4** | 移除不必要特征 |
| `learning_rate` | 0.0005 | **0.001** | 小网络可用更高学习率 |
| `epsilon_end` | 0.1 | **0.01** | 减少最终探索率 |
| `epsilon_decay` | 0.997 | **0.995** | 稍快衰减 |
| `target_update_freq` | 200 | **100** | 更频繁更新 |
| `use_double_dqn` | False | **True** | 减少过估计 |

---

### v5.0 设计哲学

**核心思想: 简单就是最好的**

```
v1-v4 的尝试:
  复杂状态 + 复杂奖励 → 学习信号混乱 → 策略不稳定

v5.0 的方法 (来自开源项目):
  简单状态 + 简单奖励 + 稳定算法 → 清晰信号 → 稳定学习
```

**关键改进:**
1. **Double DQN** - 减少 Q 值过估计
2. **Huber Loss** - 训练更稳定
3. **Urgency Signal** - 更好的状态表示
4. **极简奖励** - 通过=1, 死亡=-1, 其他=0.01
5. **更快跳跃** - 更多学习机会

---

### v5.0 训练结果 (1000轮)

![Training Curves v5.0](model/training_curves_v5.png)

#### 关键指标对比

| 指标 | v4.0 (1000轮) | v5.0 (1000轮) | 提升 |
|------|---------------|---------------|------|
| **Best Score** | 210 | **460** | +119% ✅ |
| **Avg Score** | 6.4 | **27.3** | +327% ✅ |
| **Loss 趋势** | 稳定 | 稳定 (0.005-0.01) | ✅ |
| **学习趋势** | 下降 | **持续上升** | ✅ |

#### 训练曲线分析

**1. Training Scores (左上)**
- 平均分 (橙线) 从 ~15 稳定上升到 ~30
- Episode 600+ 后开始出现大量高分 (200-460)
- 没有 v4.0 的"平均分下降"问题

**2. Training Loss (右上)**
- Loss 稳定在 0.005-0.01 范围
- 初始快速下降后保持稳定
- **Huber Loss 效果明显** - 无 v3.0 的 "Loss 上升" 问题

**3. Score Distribution (右下)**
- ~680 次 0 分 (68%) - 仍有改进空间
- 但有明显的长尾分布到 460
- 10-60 分区间的频率比 v4.0 高

#### 学习进程

| 阶段 | Episode | 表现 | 说明 |
|------|---------|------|------|
| 基础学习期 | 0-400 | avg ~15, best 70 | 积累经验 |
| 开始突破 | 400-600 | best 60→130 | 策略形成 |
| 加速期 | 600-800 | best 130→380 | 快速提升 |
| 巩固期 | 800-1000 | best 380→460, avg ~27 | 策略稳定 |

#### 改进效果验证

| 改进项 | 预期效果 | 实际验证 |
|--------|----------|----------|
| Double DQN | 减少 Q 值过估计 | ✅ Loss 稳定，无过估计 |
| Huber Loss | 训练更稳定 | ✅ Loss ~0.005，非常稳定 |
| Urgency Signal | 更好的状态表示 | ✅ 学习速度更快 |
| 更快跳跃 | 更多学习机会 | ✅ 每集更多尝试 |
| 极简奖励 | 清晰学习信号 | ✅ 稳定上升趋势 |

#### 结论

**v5.0 是目前最成功的版本：**
1. Best score 翻倍以上 (210→460)
2. 平均分提升 4 倍 (6.4→27.3)
3. 训练稳定，无退化现象
4. 所有开源项目的改进都得到验证

**仍可改进的方向：**
1. 0 分回合仍占 68% - 可能需要更多训练
2. 可尝试 2000+ 轮训练
3. 或使用 Prioritized Experience Replay

---

### v5.0 训练结果 (2000轮) - 灾难性遗忘分析

![Training Curves v5.0 2000ep](model/training_curves_v5_2000ep.png)

#### 关键指标

| 指标 | 1000轮 | 2000轮 | 变化 |
|------|--------|--------|------|
| **Best Score** | 460 | **710** | +54% ✅ |
| **Peak Avg** | 27.3 | **39.3** (ep690) | +44% ✅ |
| **Final Avg** | 27.3 | **11.1** | -59% ❌ |
| **0分回合** | 68% | **80%** | 更差 ❌ |

#### 问题诊断：灾难性遗忘 (Catastrophic Forgetting)

**现象：**
- Episode 690 达到峰值 avg=39.3
- Episode 1280 达到最高分 710
- 但 Episode 1500+ 后性能急剧下降
- 最终 avg 只有 11.1，比 1000 轮时更差

**根本原因分析：**

```
Episode 0-700:    学习期 → avg 上升到 39
Episode 700-950:  Epsilon 降到 0.01 → 几乎停止探索
Episode 950+:     只利用不探索 → 偶尔失败
                  ↓
          失败经验进入 Replay Buffer
                  ↓
          好经验被挤出 Buffer (大小=10000)
                  ↓
          网络在坏经验上训练 → 忘记好策略
                  ↓
          更多失败 → 更多坏经验 → 恶性循环
```

**关键问题：**
1. **Replay Buffer 太小** (10000) - 好经验被快速覆盖
2. **Epsilon 衰减太快** - 0.995 导致 ep950 后几乎无探索
3. **无优先级采样** - 坏经验和好经验同等对待
4. **无早停机制** - 即使性能下降仍继续训练

#### 学习曲线详细分析

| 阶段 | Episode | Avg Score | Best | 说明 |
|------|---------|-----------|------|------|
| 学习期 | 0-400 | 12→14 | 90 | 积累经验 |
| 突破期 | 400-700 | 14→39 | 260 | 快速提升 |
| **巅峰期** | 690 | **39.3** | 260 | 最佳平均分 |
| 高分期 | 700-1300 | 39→29 | **710** | 出现最高分但 avg 开始下降 |
| 遗忘期 | 1300-1700 | 29→12 | - | 性能退化 |
| 崩溃期 | 1700-2000 | 12→11 | - | 完全遗忘 |

#### 解决方案 (未来改进)

| 问题 | 解决方案 | 预期效果 |
|------|----------|----------|
| Buffer 太小 | 增大到 50000+ | 保留更多好经验 |
| Epsilon 衰减太快 | 使用更慢的衰减 (0.999) | 保持探索 |
| 无优先级采样 | Prioritized Experience Replay | 多学好经验 |
| 无早停 | 当 avg 连续下降时停止 | 避免过度训练 |
| 单一模型 | 使用 best_model 而非 final_model | 保留最佳策略 |

#### 结论

**v5.0 (2000轮) 揭示了 DQN 的一个重要问题：**
- 长时间训练不一定更好
- 需要机制来防止遗忘好策略
- **实际使用应加载 best_model.pth (score=710)**

---

### 版本对比总结

| 指标 | v1.0 | v2.0 | v3.0 | v4.0 | v5.0 (2000ep) |
|------|------|------|------|------|---------------|
| 状态维度 | 8 | 11 | 5 | 5 | **4** |
| 动作数 | 3 | 3 | 2 | 2 | 2 |
| 死亡惩罚 | -100 | -100 | -10 | -1 | **-1** |
| 通过奖励 | +10 | +25 | +10 | +5 | **+1** |
| Double DQN | ❌ | ❌ | ❌ | ❌ | **✅** |
| Huber Loss | ❌ | ❌ | ❌ | ❌ | **✅** |
| 最高分 | 100 | 130 | 110 | 210 | **710** |
| 峰值平均分 | 15.9 | 24.6 | 11.8 | 9.4 | **39.3** |
| 学习趋势 | 无 | 无 | 下降 | 不稳定 | **上升后遗忘** |

---

## Version 4.0 - 纯正向奖励版 (已废弃)

**改动日期:** 2026-02-05

**问题诊断（基于 v3.0 训练曲线分析）:**

![Training Curves v3.0](model/training_curves_v3.png)

**严重问题：**
1. **Loss 上升**: 从1-2上升到5-7，网络在"遗忘"而不是学习
2. **平均分下降**: 从初期20分下降到后期12分，性能退化
3. **大量0分回合**: 超过200次0分，智能体在第一个障碍物就死了

**根本原因分析:**
- v3.0 中惩罚"错误跳跃"(-0.5)
- 智能体学到: 跳跃有风险 → 不如不跳
- 但不跳 = 必死 → 陷入死循环

---

### 改动 1: 移除所有跳跃惩罚

**文件:** `game/dino_game.py` - `_calculate_reward()` 方法

**之前版本 (v3.0):**
```python
if is_close:
    if action == 1 and was_on_ground:
        reward += 1.0   # 正确跳跃奖励
else:  # 障碍物远
    if action == 1 and was_on_ground:
        reward -= 0.5   # 错误跳跃惩罚 ← 问题根源！
```

**现在版本 (v4.0):**
```python
# 只奖励，不惩罚
if is_close and action == 1 and was_on_ground:
    reward += 0.5  # 正确跳跃奖励

# 完全移除错误跳跃的惩罚
```

**之前版本的缺点:**
1. 惩罚让智能体"害怕"跳跃
2. 智能体学会了保守策略（不跳）
3. 不跳必死，形成恶性循环

**改动原因:**
1. **正向强化原则**: 只奖励好行为，不惩罚探索
2. **鼓励尝试**: 让智能体敢于跳跃
3. **参考 RL 最佳实践**: 避免稀疏负奖励

---

### 改动 2: 大幅降低死亡惩罚

**之前版本 (v3.0):**
```python
if collision:
    return -10.0  # 大惩罚
```

**现在版本 (v4.0):**
```python
if collision:
    return -1.0   # 小惩罚
```

**之前版本的缺点:**
1. -10 相对于 +0.1 的存活奖励太大
2. 智能体过度避险，不敢尝试新策略
3. Q值范围过大，数值不稳定

**改动原因:**
1. **平衡的奖励幅度**: 死亡(-1) vs 通过(+5) vs 存活(+0.1)
2. **鼓励探索**: 死亡不可怕，重要的是学习
3. **数值稳定**: 更小的奖励范围

---

### 改动 3: 降低游戏难度

**文件:** `game/constants.py`

| 参数 | v3.0 | v4.0 | 改动原因 |
|------|------|------|----------|
| `OBSTACLE_WIDTH` | 30 | 25 | 更窄的障碍物更容易跳过 |
| `OBSTACLE_MIN_HEIGHT` | 40 | 35 | 更矮的障碍物 |
| `OBSTACLE_MAX_HEIGHT` | 70 | 55 | 限制最大高度 |
| `OBSTACLE_SPEED_INIT` | 8 | 6 | 更慢的初始速度 |
| `OBSTACLE_SPEED_MAX` | 15 | 12 | 限制最大速度 |
| `OBSTACLE_GAP_MIN` | 300 | 400 | 更大的障碍物间距 |
| `OBSTACLE_GAP_MAX` | 600 | 700 | 更多反应时间 |

**之前版本的缺点:**
1. 游戏太难，智能体很难获得正向反馈
2. 障碍物来得太快，没有足够时间学习
3. 高难度 + 低成功率 = 学习信号稀疏

**改动原因:**
1. **课程学习思想**: 先学简单的，再逐渐增加难度
2. **更多正向反馈**: 降低难度 = 更容易成功 = 更多奖励
3. **建立基础技能**: 先学会基本跳跃，再优化时机

---

### 改动 4: 调整训练参数

| 参数 | v3.0 | v4.0 | 改动原因 |
|------|------|------|----------|
| `learning_rate` | 0.001 | 0.0005 | 更稳定的学习 |
| `gamma` | 0.99 | 0.95 | 更关注即时奖励 |
| `epsilon_end` | 0.05 | 0.1 | 保留更多探索 |
| `epsilon_decay` | 0.995 | 0.997 | 更慢的衰减 |
| `batch_size` | 64 | 32 | 更频繁的更新 |
| `buffer_size` | 50000 | 20000 | 更新鲜的经验 |
| `target_update_freq` | 100 | 200 | 更稳定的目标 |

**改动原因:**
1. **更小的 batch**: 更频繁更新，更快响应新经验
2. **更小的 buffer**: 避免过时经验影响学习
3. **更高的 epsilon_end**: 保持 10% 探索，避免陷入局部最优
4. **更低的 gamma**: 更关注"当前跳跃能否成功"

---

### v4.0 设计哲学

**核心思想: 正向强化 + 降低难度**

```
v1-v3 的错误思路:
  "惩罚错误行为" → 智能体害怕尝试 → 不学习

v4.0 的正确思路:
  "奖励正确行为" → 智能体敢于尝试 → 逐渐学会
```

**奖励设计原则:**
- 通过障碍: +5 (主要目标)
- 正确跳跃: +0.5 (引导行为)
- 存活: +0.1 (基础奖励)
- 死亡: -1 (小惩罚，不可怕)
- 错误跳跃: 0 (不惩罚，允许探索)

---

### v4.0 训练结果 (500轮)

![Training Curves v4.0](model/training_curves_v4_500ep.png)

**重要发现 - "顿悟"现象 (Aha Moment):**

| 训练阶段 | Episode | 表现 | 分析 |
|---------|---------|------|------|
| 积累期 | 1-400 | 几乎全是0分 | 智能体在积累经验，随机探索 |
| 突破期 | 400-450 | 突然出现70→100→210 | "顿悟"发生，学会了跳跃时机 |
| 稳定期 | 450-500 | 平均分上升到9.4 | 策略开始稳定 |

**关键指标:**
- Best Score: **210** (vs v3.0的110)
- 最长存活: **839步** (Episode 480)
- Loss: 从0上升到0.15（正常，代表在学习）

**结论:**
1. v4.0 的"纯正向奖励"策略正确
2. 智能体需要 ~400轮 来积累足够经验
3. 继续训练可能获得更好效果

---

### 版本对比

| 指标 | v1.0 | v2.0 | v3.0 | v4.0 |
|------|------|------|------|------|
| 动作数 | 3 | 3 | 2 | 2 |
| 状态维度 | 8 | 11 | 5 | 5 |
| 死亡惩罚 | -100 | -100 | -10 | **-1** |
| 错误跳跃惩罚 | 0 | -0.3 | -0.5 | **0** |
| 游戏难度 | 标准 | 标准 | 标准 | **降低** |
| 最高分 | 100 | 130 | 110 | **210** |
| 学习趋势 | 无 | 无 | 下降 | **上升** |

---

## Version 3.0 - 简化版 (已废弃)

**改动日期:** 2026-02-05

**问题诊断（基于 v2.0 训练曲线分析）:**

![Training Curves v2.0](model/training_curves_v2.png)

1. **Score曲线问题**: 平均分只从15升到25，波动极大，没有稳定上升趋势
2. **Loss下降但分数不升**: Loss从100降到20，但分数没提升 → 网络在学习错误的目标
3. **分数分布**: 大部分集中在0-20分，高分极少 → 智能体没学会稳定的跳跃策略

---

### 改动 1: 大幅简化动作空间

**文件:** `game/dino_game.py`

**之前版本 (v2.0):**
```python
# 3个动作
action_size = 3  # 0=不动, 1=跳跃, 2=下蹲
```

**现在版本 (v3.0):**
```python
# 2个动作
action_size = 2  # 0=不跳, 1=跳跃
```

**之前版本的缺点:**
1. 下蹲动作在没有飞行障碍时完全无用
2. 3个动作增加了探索空间，延长了学习时间
3. 智能体可能学会用"下蹲"来逃避，而不是学习正确的跳跃时机

**改动原因:**
1. **奥卡姆剃刀**: 最简单的解决方案往往是最好的
2. 减少动作空间 = 减少需要学习的策略数量
3. 让智能体专注于学习"何时跳跃"这一个核心技能

---

### 改动 2: 大幅简化状态空间

**文件:** `game/dino_game.py` - `get_state()` 方法

**之前版本 (v2.0):**
```python
# 11维状态向量
state = np.zeros(11, dtype=np.float32)
# 包含大量冗余信息
```

**现在版本 (v3.0):**
```python
# 5维状态向量 - 只保留核心信息
state = np.zeros(5, dtype=np.float32)

state[0] = dist / 300           # 到障碍物的距离 (0-1)
state[1] = height / MAX_HEIGHT  # 障碍物高度
state[2] = 1.0 if dist < 150    # 是否在跳跃区域 (二值)
state[3] = 1.0 if jumping       # 是否在空中 (二值)
state[4] = speed / MAX_SPEED    # 当前速度
```

**之前版本的缺点:**
1. 11维状态包含大量冗余/噪声信息
2. 神经网络需要更多数据来学习有用特征
3. 部分特征可能互相矛盾，混淆学习

**改动原因:**
1. **信息精简**: 只保留做跳跃决策真正需要的5个特征
2. **清晰的二值特征**: `is_close` 和 `is_jumping` 用0/1表示，更容易学习
3. **减少网络负担**: 更小的输入 = 更快的收敛

---

### 改动 3: 简化奖励函数

**文件:** `game/dino_game.py` - `_calculate_reward()` 方法

**之前版本 (v2.0):**
```python
def _calculate_reward(self, action, collision, passed):
    # 复杂的距离相关奖励
    if dist > 200: reward -= 0.3
    elif 50 < dist < 150: reward += 1.0
    elif 0 < dist < 50: reward += 2.0
    # ... 多种条件判断
```

**现在版本 (v3.0):**
```python
def _calculate_reward(self, action, collision, passed, old_state):
    if collision: return -10.0      # 死亡惩罚
    if passed > 0: return 10.0      # 通过奖励

    reward = 0.1  # 存活奖励

    # 简单的跳跃时机奖励
    if is_close and action == 1 and was_on_ground:
        reward += 1.0   # 正确时机跳跃
    elif not is_close and action == 1 and was_on_ground:
        reward -= 0.5   # 错误时机跳跃

    return reward
```

**之前版本的缺点:**
1. 奖励规则太复杂，智能体难以理解因果关系
2. 多个距离区间的奖励可能互相冲突
3. 奖励信号噪声大，难以收敛

**改动原因:**
1. **清晰的因果关系**: 只有两种情况 - 正确跳跃(+1) vs 错误跳跃(-0.5)
2. **减少奖励噪声**: 简单规则 = 更清晰的学习信号
3. **参考 old_state**: 判断动作前的状态，避免奖励延迟问题

---

### 改动 4: 调整奖励幅度

**之前版本 (v2.0):**
```python
collision_penalty = -100.0
pass_reward = +25.0
```

**现在版本 (v3.0):**
```python
collision_penalty = -10.0
pass_reward = +10.0
```

**之前版本的缺点:**
1. -100 的惩罚太大，导致 Q 值范围过大
2. 奖励幅度不平衡，智能体过度避险
3. 可能导致数值不稳定

**改动原因:**
1. **平衡的奖励**: 死亡(-10) 和 通过(+10) 幅度相等
2. **数值稳定**: 更小的奖励范围，更稳定的训练
3. **合理的风险收益比**: 不会过度惧怕死亡而不敢行动

---

### 改动 5: 恢复部分超参数

| 参数 | v2.0 | v3.0 | 改动原因 |
|------|------|------|----------|
| `state_size` | 11 | 5 | 简化状态 |
| `action_size` | 3 | 2 | 简化动作 |
| `learning_rate` | 0.0005 | 0.001 | 简单问题可以用更大学习率 |
| `epsilon_end` | 0.01 | 0.05 | 保留更多探索 |
| `epsilon_decay` | 0.998 | 0.995 | 适中的衰减速度 |
| `batch_size` | 128 | 64 | 简单问题不需要大batch |
| `buffer_size` | 100000 | 50000 | 减少内存占用 |

---

### 版本对比

| 指标 | v1.0 | v2.0 | v3.0 |
|------|------|------|------|
| 状态维度 | 8 | 11 | **5** |
| 动作数量 | 3 | 3 | **2** |
| 奖励函数 | 简单稀疏 | 复杂密集 | **简单密集** |
| 死亡惩罚 | -100 | -100 | **-10** |
| 最高分 | 100 | 130 | 待测试 |

---

### 设计哲学变化

**v1.0/v2.0 的问题**: 试图通过增加信息量来帮助智能体
- 更多状态特征
- 更复杂的奖励函数
- 结果: 信息过载，学习困难

**v3.0 的思路**: 简化问题本身
- 最少的必要信息
- 最清晰的奖励信号
- 核心: 让智能体学会"什么时候跳"这一件事

---

## Version 2.0 - 改进版 (已废弃)

**改动日期:** 2026-02-05

---

### 改动 1: 状态空间扩展

**文件:** `game/dino_game.py` - `get_state()` 方法

**之前版本 (v1.0):**
```python
# 8维状态向量
state = np.zeros(8, dtype=np.float32)
# 包含: 距离、高度、宽度、速度、位置、是否跳跃、是否飞行、第二障碍距离
```

**现在版本 (v2.0):**
```python
# 11维状态向量
state = np.zeros(11, dtype=np.float32)
# 新增特征:
# - state[4]: 是否在危险区域 (距离<100)
# - state[5]: 是否在跳跃区域 (50<距离<200)
# - state[9]: 恐龙垂直速度
```

**之前版本的缺点:**
1. 状态信息不够丰富，智能体难以判断精确的跳跃时机
2. 没有明确的"危险区域"概念，智能体不知道何时必须跳跃
3. 缺少速度信息，无法预测跳跃轨迹

**改动原因:**
1. 添加"危险区域"和"跳跃区域"的布尔特征，帮助智能体学习关键决策点
2. 添加恐龙垂直速度，让智能体能预测落地时间
3. 更丰富的状态表示有助于神经网络学习更精确的策略

---

### 改动 2: 奖励函数重设计

**文件:** `game/dino_game.py` - 新增 `_calculate_reward()` 方法

**之前版本 (v1.0):**
```python
def step(self, action):
    reward = 0.1  # 存活奖励

    if passed > 0:
        reward += 10 * passed  # 通过障碍

    if collision:
        reward = -100  # 碰撞惩罚
```

**现在版本 (v2.0):**
```python
def _calculate_reward(self, action, collision, passed):
    reward = 0.0

    # 1. 碰撞惩罚
    if collision:
        return -100.0

    # 2. 通过障碍奖励 (提高到25)
    if passed > 0:
        reward += 25.0 * passed

    # 3. 存活奖励
    reward += 0.1

    # 4. 距离相关的奖励塑形 (新增)
    if nearest_obs:
        dist = nearest_obs.x - (self.dino.x + self.dino.width)

        if dist > 200:
            # 远离障碍时跳跃 -> 惩罚
            if self.dino.is_jumping:
                reward -= 0.3

        elif 50 < dist < 150:
            # 跳跃区域，在空中 -> 奖励
            if self.dino.is_jumping:
                reward += 1.0

        elif 0 < dist < 50:
            # 危险区域，在空中 -> 大奖励
            if self.dino.is_jumping:
                reward += 2.0
            else:
                reward -= 0.5  # 没跳 -> 惩罚

    return reward
```

**之前版本的缺点:**
1. **奖励稀疏**: 只有通过障碍或死亡才有明显奖励，中间过程没有反馈
2. **缺乏引导**: 智能体不知道什么时候应该跳跃
3. **无法学习时机**: 没有针对跳跃时机的奖励，智能体可能乱跳或不跳

**改动原因:**
1. **奖励塑形 (Reward Shaping)**: 参考 sample.docx 中的方法，根据与障碍物的距离给予不同奖励
2. **鼓励正确行为**: 在跳跃区域跳跃给予奖励，在远处乱跳给予惩罚
3. **提供密集反馈**: 让智能体每一步都能获得有意义的反馈，加速学习

---

### 改动 3: 超参数调整

**文件:** `agent/agent.py`, `train.py`

| 参数 | v1.0 | v2.0 | 改动原因 |
|------|------|------|----------|
| `state_size` | 8 | 11 | 配合扩展的状态空间 |
| `learning_rate` | 0.001 | 0.0005 | 降低学习率，训练更稳定 |
| `gamma` | 0.95 | 0.99 | 提高折扣因子，更重视长期奖励 |
| `epsilon_decay` | 0.995 | 0.998 | 减慢衰减，延长探索期 |
| `batch_size` | 64 | 128 | 增大批次，梯度估计更稳定 |
| `target_update_freq` | 100 | 50 | 更频繁更新目标网络 |

**之前版本的缺点:**

1. **学习率过高 (0.001)**
   - 训练不稳定，loss 波动大
   - 可能跳过最优解

2. **Epsilon 衰减过快 (0.995)**
   - 500轮后 epsilon ≈ 0.08，探索几乎停止
   - 智能体可能陷入局部最优

3. **Gamma 偏低 (0.95)**
   - 对未来奖励折扣太多
   - 智能体可能过于短视，不注重长期存活

4. **Batch Size 偏小 (64)**
   - 梯度估计方差大
   - 训练不够稳定

**改动原因:**
1. 降低学习率使训练更平滑
2. 减慢 epsilon 衰减，让智能体有更多时间探索
3. 提高 gamma，让智能体更重视长期生存
4. 增大 batch size，获得更稳定的梯度估计

---

### 改动 4: 简化游戏难度

**文件:** `game/dino_game.py` - `_spawn_obstacle()` 方法

**之前版本 (v1.0):**
```python
def _spawn_obstacle(self):
    # 20% 概率生成飞行障碍
    is_flying = self.score > 100 and random.random() < 0.2
    obstacle = Obstacle(x, self.speed, is_flying)
```

**现在版本 (v2.0):**
```python
def _spawn_obstacle(self):
    # 暂时移除飞行障碍，简化学习
    is_flying = False
    obstacle = Obstacle(x, self.speed, is_flying)
```

**之前版本的缺点:**
1. 飞行障碍需要下蹲或跳跃两种策略
2. 增加了动作空间的复杂度
3. 智能体需要同时学习两种不同的躲避策略，学习难度大

**改动原因:**
1. **渐进式学习**: 先让智能体学会基本的跳跃躲避
2. **降低复杂度**: 减少需要学习的策略数量
3. **后续可扩展**: 等基本跳跃学好后，可以重新加入飞行障碍

---

### 改动 5: 障碍物通过检测优化

**文件:** `game/dino_game.py` - `step()` 方法

**之前版本 (v1.0):**
```python
# 障碍物移出屏幕左侧时计数
for obs in self.obstacles:
    if obs.x + obs.width < 0:
        passed += 1
```

**现在版本 (v2.0):**
```python
# 障碍物通过恐龙时计数（更精确）
for obs in self.obstacles:
    if obs.x + obs.width < self.dino.x and not obs.passed:
        obs.passed = True
        passed += 1
```

**之前版本的缺点:**
1. 障碍物要完全离开屏幕才计算通过
2. 奖励反馈延迟太大（障碍物从通过到离开屏幕可能有几秒）
3. 智能体难以将跳跃动作与通过奖励关联起来

**改动原因:**
1. **及时反馈**: 障碍物一通过恐龙就立即给予奖励
2. **强化关联**: 让跳跃动作和奖励的时间关联更紧密
3. **添加 `passed` 标记**: 防止同一障碍物被重复计数

---

## Version 1.0 - 初始版本

**创建日期:** 2026-02-05

### 初始实现

- 基于 Chrome Dino 游戏的平台跳跃环境
- 8维状态空间
- 3个动作 (不动、跳跃、下蹲)
- 基本 DQN 实现
- 简单奖励函数

### 训练结果

```
Episode  500 | Score:    10 | Avg Score:   15.9 | Epsilon: 0.082
Best score: 100
```

### 问题分析

1. 平均分数在 15-20 之间波动，没有明显上升趋势
2. 最高分只有 100，说明智能体没有学会稳定的跳跃策略
3. Epsilon 衰减到 0.08 后，探索停止，但策略未收敛

---

## 版本对比总结

| 指标 | v1.0 | v2.0 (预期) |
|------|------|-------------|
| 状态维度 | 8 | 11 |
| 奖励类型 | 稀疏 | 密集 |
| 训练稳定性 | 波动大 | 更稳定 |
| 探索时间 | 短 | 长 |
| 最高分 | 100 | 待测试 |

---

## 后续优化方向

如果 v2.0 效果仍不理想，可以考虑：

1. **Double DQN**: 减少 Q 值过估计
2. **Dueling DQN**: 分离状态价值和动作优势
3. **Prioritized Experience Replay**: 优先采样重要经验
4. **增加训练轮数**: 2000-5000 轮
5. **调整网络结构**: 增加层数或神经元
6. **课程学习**: 从慢速开始，逐渐增加游戏速度
