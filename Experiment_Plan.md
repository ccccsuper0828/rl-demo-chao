# 实验方案 / Experiment Plan

## 实验目标
训练一个 DQN 智能体学会玩 Dino Jump 平台跳跃游戏，并评估其性能。

---

## 实验步骤

### 第一阶段：环境验证 (Day 1)

**步骤 1: 测试游戏环境**
```bash
cd /Users/chaowang/Assignment1_RL_Game
python play.py --mode human
```
- [ ] 确认游戏窗口正常显示
- [ ] 确认跳跃 (SPACE) 和下蹲 (S) 操作正常
- [ ] 确认碰撞检测正常
- [ ] 确认分数计算正常
- [ ] 截图保存游戏界面 (用于报告 Fig 2.1-2.3)

**步骤 2: 测试模型模块**
```bash
python -c "from agent import DQNAgent; a = DQNAgent(); print('Agent OK')"
```
- [ ] 确认 DQN 网络可以正常创建
- [ ] 确认可以在 CPU 上运行

---

### 第二阶段：基线训练 (Day 2-3)

**步骤 3: 运行基线训练**
```bash
python train.py --episodes 500
```

**记录以下数据：**

| 训练阶段 | Episodes | 平均分数 | Epsilon | 备注 |
|---------|----------|---------|---------|------|
| 初期 | 1-100 | | | |
| 中期 | 100-300 | | | |
| 后期 | 300-500 | | | |
| 最终 | 500 | | 0.01 | |

- [ ] 保存 `model/training_curves.png`
- [ ] 记录最高分数
- [ ] 记录训练耗时

**步骤 4: 评估基线模型**
```bash
python play.py --mode ai --games 10
```

记录 10 次游戏的分数：
| 游戏 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 平均 |
|-----|---|---|---|---|---|---|---|---|---|----|----|
| 分数 | | | | | | | | | | | |

---

### 第三阶段：参数调优 (Day 4-5)

**实验 A: 学习率对比**

| 学习率 | 500轮后平均分 | 收敛速度 |
|--------|-------------|---------|
| 0.0001 | | |
| 0.001 (基线) | | |
| 0.01 | | |

修改 `agent/agent.py` 中的 `learning_rate` 参数

**实验 B: 折扣因子对比**

| Gamma | 500轮后平均分 | 备注 |
|-------|-------------|------|
| 0.9 | | |
| 0.95 (基线) | | |
| 0.99 | | |

**实验 C: 网络结构对比 (可选)**

| 网络结构 | 参数量 | 平均分 |
|---------|-------|-------|
| 256-128-64 (基线) | | |
| 128-64-32 | | |
| 512-256-128 | | |

---

### 第四阶段：最终评估 (Day 6)

**步骤 5: 选择最佳模型**
- [ ] 比较各实验结果
- [ ] 选择表现最好的参数组合
- [ ] 重新训练最终模型

**步骤 6: 性能测试**
```bash
python play.py --mode ai --games 20
```

**最终性能指标：**
- 平均分数: ____
- 最高分数: ____
- 最低分数: ____
- 标准差: ____

**步骤 7: 人机对比**
```bash
python play.py --mode compare
```

| 对比 | AI 分数 | 人类分数 | 胜者 |
|-----|--------|---------|-----|
| 第1局 | | | |
| 第2局 | | | |
| 第3局 | | | |

---

### 第五阶段：文档整理 (Day 7)

**步骤 8: 截图收集**
- [ ] 游戏开始界面截图
- [ ] 游戏进行中截图
- [ ] Game Over 截图
- [ ] 训练曲线图 (training_curves.png)
- [ ] (可选) 奖励函数代码截图

**步骤 9: 完成报告**
- [ ] 填写 Report.md 中的实验数据
- [ ] 添加截图到报告
- [ ] 检查字数 (1000-1500 词)
- [ ] 导出为 Word/PDF 格式

**步骤 10: 录制视频**
视频内容建议 (3-5分钟)：
1. 项目介绍 (30秒)
2. 游戏演示 - 人类玩 (30秒)
3. AI 训练过程说明 (1分钟)
4. AI 演示 - 展示训练效果 (1分钟)
5. 代码结构讲解 (1分钟)
6. 挑战与解决方案 (30秒)
7. 总结 (30秒)

**步骤 11: 上传 GitHub**
```bash
git init
git add .
git commit -m "Initial commit: Dino Jump DQN project"
git remote add origin [your-repo-url]
git push -u origin main
```

---

## 时间规划

| 日期 | 任务 | 预计时间 |
|-----|------|---------|
| Day 1 | 环境验证 + 熟悉代码 | 2小时 |
| Day 2 | 基线训练 | 3小时 |
| Day 3 | 评估 + 调试 | 2小时 |
| Day 4 | 参数调优实验 | 3小时 |
| Day 5 | 继续调优 + 分析 | 2小时 |
| Day 6 | 最终评估 | 2小时 |
| Day 7 | 报告 + 视频 + 上传 | 4小时 |

**总计：约 18 小时**

---

## 常见问题排查

### Q: 训练时分数没有提升？
A: 检查以下几点：
1. 确认 epsilon 在正确衰减
2. 检查 replay buffer 是否有足够样本
3. 尝试调整学习率

### Q: 游戏运行很卡？
A:
1. 训练时不要用 `--render` 参数
2. 减少 FPS (在 constants.py 中)

### Q: 模型训练后效果不好？
A:
1. 增加训练轮数到 1000
2. 调整奖励函数
3. 检查状态表示是否合理

---

## 检查清单 (Deliverables)

- [ ] 代码完整可运行
- [ ] model/best_model.pth 存在
- [ ] model/training_curves.png 存在
- [ ] Report (Word/PDF, 1000-1500词)
- [ ] GitHub 仓库已创建
- [ ] YouTube 视频已上传
- [ ] 所有文件放在 "Assignment 1 - [Your Name]" 文件夹

**截止日期：2026年3月3日 晚上8点**
