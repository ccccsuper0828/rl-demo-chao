% Dino Jump Game with Deep Q-Learning - Technical Report
% Author: Chao Wang
% Date: February 2026

\documentclass[11pt,a4paper]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\geometry{margin=2.5cm}

% Math
\usepackage{amsmath}
\usepackage{amssymb}

% Graphics
\usepackage{graphicx}
\usepackage{float}

% Tables
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\usepackage{xcolor}

% References and links
\usepackage{hyperref}
\usepackage{url}

% Code listing style
\definecolor{codebg}{RGB}{245,245,245}
\definecolor{codecomment}{RGB}{60,128,49}
\definecolor{codestring}{RGB}{163,21,21}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{codebg},
    commentstyle=\color{codecomment},
    stringstyle=\color{codestring},
    keywordstyle=\color{blue}\bfseries,
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false
}

% Title
\title{\textbf{Dino Jump Game Based on Deep Q-Learning}}
\author{Chao Wang}
\date{February 2026}

\begin{document}

\maketitle

Github: \url{https://github.com/chaowang/Assignment1_RL_Game}

% ======================================================================
\section{Introduction}
% ======================================================================

In this project, I used the Deep Q-Network (DQN) algorithm to design and implement a Dino Jump game. In the game, an AI-controlled Dino character learns to jump over approaching obstacles autonomously. The Dino is controlled by a DQN agent and makes decisions based on the current game state, continuously optimizing its strategy through reinforcement learning. I used Python as the main programming language, PyTorch for building the neural network, and Pygame for the game environment. The project also implements Double DQN and Experience Replay techniques to improve training stability and data efficiency.

% ======================================================================
\section{Game Design}
% ======================================================================

% ----------------------------------------------------------------------
\subsection{Rules of the Game}
% ----------------------------------------------------------------------

In this game, the AI-controlled Dino character must jump over obstacles that approach from the right side of the screen. The Dino starts on the ground and has two actions: jump or do nothing. Obstacles spawn at random intervals and heights, and each successfully passed obstacle awards 10 points. The game ends when the Dino collides with an obstacle. The difficulty gradually increases as obstacles move faster over time.

The state space of the game includes 6 features: the normalized distance to the nearest obstacle, an urgency signal indicating when to jump, the current jumping state (airborne or grounded), the vertical velocity, the current obstacle speed, and the nearest obstacle height. The speed and height features are critical because the optimal jump timing is speed-dependent---at higher speeds, the agent must jump earlier and from a greater distance.

% ----------------------------------------------------------------------
\subsection{Class Design of the Game}
% ----------------------------------------------------------------------

\textbf{DinoGame class} is responsible for managing the overall flow and state of the game. It initializes the game window and game objects, provides the state observations, calculates rewards, and handles collision detection.

\textbf{DQNAgent class} implements the reinforcement learning agent. It manages both the online and target neural networks, implements the epsilon-greedy exploration strategy, stores experiences in the replay buffer, and performs training updates using the Bellman equation.

\textbf{DQN class} represents the Q-value function approximator. It is a 3-layer fully connected network (6$\to$128$\to$64$\to$2) with ReLU activations and Kaiming initialization, containing approximately 9,000 parameters.

\textbf{Dino class} represents the player character. It handles jumping mechanics with realistic physics (jump velocity $-18$, gravity $1.2$) and provides collision detection.

\textbf{Obstacle class} represents the obstacles in the game. It handles obstacle movement, random height generation, and collision detection.

\textbf{ReplayBuffer class} implements a circular buffer for storing past experiences and supports random sampling for mini-batch training.

% ----------------------------------------------------------------------
\subsection{UI Design}
% ----------------------------------------------------------------------

The user interface follows a minimalist style. The game uses Pygame to render a simple Dino character, green rectangular obstacles, a ground line, and a real-time score display. During training, rendering can be disabled to speed up the learning process significantly.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/v6.1_clean_training.png}
\caption{Training curves of the final v6.1 Clean model showing the ``aha moment'' at episodes 580--680.}
\label{fig:training}
\end{figure}

% ======================================================================
\section{Implementation of Deep Q-Learning Algorithm}
% ======================================================================

Q-Learning is a value-based reinforcement learning algorithm that selects the best action by learning a Q-value function. In my project, since the game state is continuous (distance, velocity, etc.), I used a Deep Q-Network (DQN) to approximate the Q-value function instead of a discrete Q-table. I also implemented Double DQN, which uses the online network to select actions and the target network to evaluate them, reducing the problem of Q-value overestimation:

\begin{equation}
Q_{\text{target}} = r + \gamma \cdot Q_{\text{target}}(s', \arg\max_{a'} Q_{\text{online}}(s', a'))
\end{equation}

I defined a \texttt{get\_state} function to obtain the current state during training. The state vector consists of 6 features: the normalized distance to the nearest obstacle (0--1), an urgency signal that increases as the obstacle approaches, a binary jumping indicator, the normalized vertical velocity, the current obstacle speed (normalized by max speed), and the nearest obstacle height (normalized). Including speed and height is essential: the optimal jump distance changes from $\sim$42\,px at speed~5 to $\sim$115\,px at speed~10, so without this information the agent cannot learn speed-dependent timing.

During training, the agent selects actions using the $\varepsilon$-greedy strategy to balance exploration and exploitation. The epsilon starts at 1.0 (fully random) and decays by a factor of 0.995 per episode down to a minimum of 0.01. I found that this decay rate is critical---it creates the right balance for what I call the ``aha moment,'' a sudden performance breakthrough that typically occurs around episodes 600--700.

I set the following training parameters: learning rate 0.0005, discount factor 0.95, batch size 64, replay buffer size 10,000, and target network update frequency of every 100 steps. The loss function is Huber Loss (SmoothL1), and I use the Adam optimizer.

The reward function went through two important iterations. My initial version simply gave $+1$ for passing an obstacle, $-1$ for collision, and $+0.01$ per frame survived. However, I noticed the agent learned to \textbf{jump non-stop} because jumping had \textbf{zero cost}---the agent received the same survival bonus regardless of whether it jumped or stayed grounded. From the agent's perspective, jumping was always risk-free, so it spammed the jump button continuously.

To solve this, I performed a physics analysis of the jump mechanics. With jump velocity $-18$ and gravity $1.2$, a full jump takes 30 frames and reaches peak height at frame~15. The Dino's bottom position at frame $f$ follows:

\begin{equation}
y_{\text{bottom}}(f) = 320 - 17.4\,f + 0.6\,f^2
\end{equation}

For the Dino to safely clear an obstacle of height $h$, I need $y_{\text{bottom}} < 320 - h$, which gives the quadratic $0.6\,f^2 - 17.4\,f + h < 0$. Solving this yields the exact safe-frame boundaries for any obstacle height:

\begin{equation}
f_{\text{first,last}} = \frac{17.4 \mp \sqrt{302.76 - 2.4\,h}}{1.2}
\end{equation}

Crucially, I found that the safe window must be \textbf{dynamically narrowed for taller obstacles}. At frame~25 with a 50\,px obstacle, the clearance is only 10\,px---barely safe. To prevent the Dino from clipping obstacles on descent, I add a height-proportional safety margin of $h/25$ frames on each side. This tightens the window significantly for tall obstacles:

\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Obstacle Height} & \textbf{Margin (frames)} & \textbf{Window at speed\,5} & \textbf{Window width} \\
\midrule
30\,px (short) & 1.2 & 16--70\,px & 54\,px \\
40\,px (medium) & 1.6 & 21--64\,px & 43\,px \\
50\,px (tall) & 2.0 & 27--59\,px & 32\,px \\
\bottomrule
\end{tabular}
\end{center}

The optimal distance for all heights is approximately $42$\,px at speed~5 and $115$\,px at speed~10. Based on this analysis I designed a \textbf{physics-informed reward}:

\begin{lstlisting}[caption={Final Reward Function with Jump-Timing Feedback}]
def _calculate_reward(collision, passed, jumped_this_frame):
    if collision:
        return -1.0               # Death penalty
    if passed > 0:
        return +1.0               # Pass obstacle
    if jumped_this_frame:
        dist, obs_h = self._get_nearest_obstacle()
        if dist > 400:
            return -0.3           # No obstacle nearby
        quality = self._jump_timing_quality(dist, obs_h)
        if quality > 0:
            return +0.4 * quality # +0.4 at optimal timing
        else:
            return +0.3 * quality # -0.3 at worst timing
    return +0.01                  # Survival bonus
\end{lstlisting}

The \texttt{\_jump\_timing\_quality} function uses the quadratic formula to compute the exact safe window for the given obstacle height, adds the height-proportional safety margin, and returns $+1$ at the optimal distance, $0$ at the window edges, and $-1$ far outside. This gives the agent a dense, height-aware learning signal: for tall obstacles the agent must jump more precisely, while short obstacles allow more room for error.

% ======================================================================
\section{Challenges and Solutions}
% ======================================================================

During the development process, I tried many optimization techniques that are commonly recommended in deep learning, but I found that most of them actually hurt performance on this simple task. Through systematic experiments across multiple versions (v1.0 to v6.3), I learned several valuable lessons.

First, I tried adding Dropout (rate=0.2) to prevent overfitting. However, I found that Dropout reduced performance by about 16\%. The reason is that my network is very small (only 8,500 parameters) and the task is simple (4D state, 2 actions), so there is no overfitting to prevent. Dropout actually weakened the network's learning capacity. After removing Dropout completely, performance recovered significantly.

Second, I implemented a 3-stage curriculum learning system (Easy $\to$ Medium $\to$ Hard), expecting it to help the agent learn more smoothly. Instead, performance dropped by 30\%. The problem was that the agent learned a ``lazy jumping'' strategy in Easy mode---obstacles were slow and far apart, so it learned to jump late. When transitioning to harder stages, this strategy completely failed, and the agent had to relearn from scratch at each transition. I solved this by removing curriculum learning entirely and training directly on the target difficulty.

Third, I tried adding reward micro-signals ($\pm 0.005$) to provide fine-grained feedback for jump timing. However, these signals were 200 times smaller than the main rewards ($\pm 1.0$) and even smaller than the random noise in Q-value estimates ($\pm 0.01$--$0.05$). They added complexity without any measurable benefit. I returned to the simple three-value reward function, which proved to be the most effective.

Finally, after the failed optimizations, I tried fixing things through hyperparameter tuning---increasing the buffer size from 10,000 to 30,000 and slowing epsilon decay from 0.995 to 0.997. This actually made things worse ($-9\%$). The larger buffer contained more outdated experiences, and the slower decay delayed the critical ``aha moment.'' I realized that hyperparameter tuning cannot fix code-level problems. The real solution was a complete code cleanup (v6.1 Clean), which removed all the harmful additions and restored the simple, proven configuration.

Building on v6.1 Clean, I then made two major improvements inspired by the DinoRunTutorial~\cite{dinotutorial} reference project. First, I expanded the state from 4 to 6 dimensions by adding obstacle speed and height, enabling the agent to learn speed-dependent jump timing. Second, I replaced the flat reward with the physics-informed reward described in Section~3. These changes (v7.0) increased the average score from 33.0 to \textbf{166.4} and the maximum score from 520 to \textbf{1180}---a \textbf{5$\times$ improvement}.

% ----------------------------------------------------------------------
\subsection{Ablation Study}
% ----------------------------------------------------------------------

To understand the contribution of each component, I conducted a systematic ablation study across multiple versions. Table~\ref{tab:versions} summarizes the performance of each version, and Table~\ref{tab:ablation} isolates the impact of individual changes.

\begin{table}[H]
\centering
\caption{Performance comparison across all versions.}
\label{tab:versions}
\begin{tabular}{lccl}
\toprule
\textbf{Version} & \textbf{Avg Score} & \textbf{Max Score} & \textbf{Key Change} \\
\midrule
v5.0 (baseline)      & 39.3 & 710 & First success, but catastrophic forgetting \\
v6.2                  & 20.1 & 190 & +Dropout +Curriculum +Micro-rewards \\
v6.3-1                & 27.7 & 260 & Disabled Dropout \& curriculum \\
v6.3-2                & 25.2 & 230 & Larger buffer, slower $\varepsilon$ decay \\
v6.1 Clean            & 33.0 & 520 & Complete cleanup, simple design \\
\textbf{v7.0 (final)} & \textbf{166.4} & \textbf{1180} & \textbf{6D state + physics-based reward} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Ablation results: impact of each component.}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Impact} & \textbf{Verdict} \\
\midrule
+ Dropout (0.2)            & $-16\%$ & Harmful \\
+ Curriculum learning      & $-30\%$ & Harmful \\
+ Reward micro-signals     & $\sim 0\%$ & No benefit \\
+ Large buffer (30K)       & $-9\%$  & Harmful \\
+ Slow $\varepsilon$ decay (0.997) & $-9\%$  & Harmful \\
All three combined (v6.2) & $-49\%$ & Catastrophic \\
+ Speed \& height in state (6D) & $+400\%$ & Essential \\
+ Physics-based jump-timing reward & $+400\%$ & Essential \\
+ Height-proportional safety margin & Stability & No forgetting \\
\bottomrule
\end{tabular}
\end{table}

The ablation study reveals a clear pattern: ``standard'' deep learning tricks (Dropout, curriculum learning) \textbf{degraded} performance on this small-scale task, while \textbf{domain-specific} improvements (physics-informed state and reward) produced dramatic gains. The final v7.0 model achieved $5\times$ the average score of v6.1 Clean and ran for 2000 episodes without catastrophic forgetting.

% ======================================================================
\section{Conclusion}
% ======================================================================

During training, I observed that the model's performance improved through several distinct phases. In the early episodes (0--400), the agent explored randomly with an average score around 15. Around episodes 400--700, the first ``aha moment'' occurred as the agent began learning the relationship between obstacle distance and jump timing, with the average climbing to~35. From episodes 700--1000, performance exploded from 35 to over 125 as the physics-based reward guided the agent toward optimal jump timing. The agent continued improving through episode 1879, reaching a peak average of 166.4 with a maximum single-game score of 1180 (118 obstacles cleared). Notably, the model ran for the full 2000 episodes without catastrophic forgetting---a major improvement over earlier versions.

Through this project, I successfully implemented a Dino Jump game based on the Deep Q-Learning algorithm. I learned two key lessons. First, for small-scale RL tasks, generic deep learning techniques (Dropout, curriculum learning) can be counterproductive---understanding the problem scale matters more than following best practices blindly. Second, \textbf{domain-specific knowledge is extremely powerful}: by analyzing the jump physics and encoding the optimal timing directly into the reward function, I achieved a $5\times$ performance improvement over the simple reward baseline. The combination of a physics-informed 6D state, height-aware jump-timing rewards, and standard DQN with Double DQN produced the best results.

% ======================================================================
\section{References}
% ======================================================================

\begin{thebibliography}{9}

\bibitem{aome510}
aome510. (2019). \textit{chrome-dino-game-rl: Deep Q-Learning for Chrome Dino Game} [Computer software]. GitHub. \url{https://github.com/aome510/chrome-dino-game-rl}

\bibitem{hfahrudin}
hfahrudin. (2020). \textit{trex-DQN: T-Rex Runner Game AI using Deep Q-Network} [Computer software]. GitHub. \url{https://github.com/hfahrudin/trex-DQN}

\bibitem{mnih2015}
Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. \textit{Nature}, 518(7540), 529--533.

\bibitem{hasselt2016}
Van Hasselt, H., Guez, A., \& Silver, D. (2016). Deep reinforcement learning with double Q-learning. \textit{Proceedings of the AAAI Conference on Artificial Intelligence}.

\bibitem{sutton2018}
Sutton, R. S., \& Barto, A. G. (2018). \textit{Reinforcement learning: An introduction} (2nd ed.). MIT Press.

\bibitem{dinotutorial}
Paperspace. (2018). \textit{DinoRunTutorial: Build an AI to play Dino Run} [Computer software]. GitHub. \url{https://github.com/Paperspace/DinoRunTutorial}

\end{thebibliography}

% ======================================================================
\appendix
\section{Training Logs}
% ======================================================================

Below are the selected training logs from each major version. These logs were recorded during actual training runs and illustrate the learning dynamics discussed in the report.

% ----------------------------------------------------------------------
\subsection{v6.2 -- Failed Optimization (Dropout + Curriculum + Micro-rewards)}
% ----------------------------------------------------------------------

\begin{lstlisting}[caption={v6.2 Training Log (early stopped at episode 950)}, numbers=none]
Episode   50 | Score:     0 | Avg:    0.0 | Eps: 0.951 | Diff: Easy
Episode  100 | Score:    10 | Avg:    2.4 | Eps: 0.904 | Diff: Easy
Episode  200 | Score:    30 | Avg:   12.8 | Eps: 0.817 | Diff: Easy
Episode  300 | Score:    50 | Avg:   18.5 | Eps: 0.739 | Diff: Easy
Episode  400 | Score:    20 | Avg:   12.1 | Eps: 0.668 | Diff: Medium  <- crash
Episode  600 | Score:    30 | Avg:   15.8 | Eps: 0.548 | Diff: Medium
Episode  800 | Score:    10 | Avg:    8.4 | Eps: 0.450 | Diff: Hard    <- crash
Episode  950 | Score:     0 | Avg:   20.1 | Eps: 0.378 | Diff: Hard
\end{lstlisting}

Performance crashed at every curriculum transition. Peak average only 20.1.

% ----------------------------------------------------------------------
\subsection{v6.3-1 -- Partial Recovery (Disabled Dropout \& Curriculum)}
% ----------------------------------------------------------------------

\begin{lstlisting}[caption={v6.3-1 Training Log (early stopped at episode 831)}, numbers=none]
Episode  100 | Score:    30 | Avg:   10.2 | Eps: 0.904
Episode  200 | Score:    80 | Avg:   16.7 | Eps: 0.817
Episode  400 | Score:   110 | Avg:   20.8 | Eps: 0.668
Episode  573 | Score:   260 | Avg:   27.7 | Eps: 0.057  <- PEAK
Episode  700 | Score:    20 | Avg:   19.2 | Eps: 0.023  <- forgetting
Episode  831 | Score:     0 | Avg:   16.3 | Eps: 0.016  <- early stopped
\end{lstlisting}

Improved over v6.2 but still suffered catastrophic forgetting after episode 573.

% ----------------------------------------------------------------------
\subsection{v6.3-2 -- Hyperparameter Tuning Failure}
% ----------------------------------------------------------------------

\begin{lstlisting}[caption={v6.3-2 Training Log (buffer=30K, decay=0.997)}, numbers=none]
Episode  100 | Score:    20 | Avg:    9.8 | Eps: 0.740
Episode  300 | Score:    40 | Avg:   14.2 | Eps: 0.547
Episode  600 | Score:    50 | Avg:   18.6 | Eps: 0.405  <- still exploring
Episode  830 | Score:   230 | Avg:   25.2 | Eps: 0.299  <- PEAK
Episode 1000 | Score:    10 | Avg:   21.7 | Eps: 0.247
\end{lstlisting}

Slower epsilon decay delayed learning. No clear ``aha moment'' occurred.

% ----------------------------------------------------------------------
\subsection{v6.1 Clean -- Simple Reward Baseline}
% ----------------------------------------------------------------------

\begin{lstlisting}[caption={v6.1 Clean Training Log (1000 episodes)}, numbers=none]
Episode  100 | Score:    40 | Avg:   16.4 | Eps: 0.904
Episode  300 | Score:    80 | Avg:   18.7 | Eps: 0.739
Episode  500 | Score:   110 | Avg:   19.8 | Eps: 0.604
Episode  584 | Score:   310 | Avg:   21.2 | Eps: 0.562  <- first avg > 20
Episode  678 | Score:   280 | Avg:   30.1 | Eps: 0.511  <- "aha moment"
Episode  987 | Score:   520 | Avg:   33.0 | Eps: 0.373  <- BEST SCORE
Episode 1000 | Score:   120 | Avg:   33.0 | Eps: 0.368  <- FINAL
\end{lstlisting}

Avg 33.0, max 520. Stable but limited by 4D state and flat reward.

% ----------------------------------------------------------------------
\subsection{v7.0 -- Physics-Based Reward (Final Version)}
% ----------------------------------------------------------------------

\begin{lstlisting}[caption={v7.0 Training Log (2000 episodes, 6D state, physics reward)}, numbers=none]
Episode  100 | Score:    10 | Avg:   16.7 | Eps: 0.606
Episode  300 | Score:    30 | Avg:    9.6 | Eps: 0.222
Episode  430 | Score:    70 | Avg:   18.7 | Eps: 0.116  <- learning starts
Episode  600 | Score:     0 | Avg:   28.0 | Eps: 0.049
Episode  700 | Score:   120 | Avg:   35.5 | Eps: 0.030  <- first "aha moment"
Episode  800 | Score:   120 | Avg:   65.1 | Eps: 0.018  <- rapid growth
Episode  860 | Score:   530 | Avg:   83.3 | Eps: 0.013
Episode  900 | Score:   130 | Avg:   92.7 | Eps: 0.011  <- avg crosses 100
Episode  981 | Score:   870 | Avg:  118.5 | Eps: 0.010
Episode 1058 | Score:   100 | Avg:  158.1 | Eps: 0.010  <- peak avg 158
Episode 1325 | Score:  1180 | Avg:  122.0 | Eps: 0.010  <- BEST SCORE
Episode 1500 | Score:    90 | Avg:  134.3 | Eps: 0.010
Episode 1780 | Score:   760 | Avg:  124.6 | Eps: 0.010  <- still high scores
Episode 1879 | Score:    60 | Avg:  166.4 | Eps: 0.010  <- PEAK AVG
Episode 2000 | Score:    90 | Avg:  136.8 | Eps: 0.010  <- FINAL (no early stop)
\end{lstlisting}

The v7.0 model achieved a $5\times$ improvement over v6.1 Clean: peak average 166.4 (vs 33.0), maximum score 1180 (vs 520). Most importantly, it ran for the full 2000 episodes without triggering early stopping---the catastrophic forgetting problem that plagued all earlier versions was completely eliminated by the physics-based reward and height-aware safety margins.

\end{document}
